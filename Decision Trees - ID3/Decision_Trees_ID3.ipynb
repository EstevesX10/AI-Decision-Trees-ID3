{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "---\n",
    "# Decision Trees - ID3 [Artificial Intelligence Project]\n",
    "---\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem Presentation\n",
    "***\n",
    "</div>\n",
    "    \n",
    "> ADD PROBLEM PRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## ID3 Algorithm \n",
    "***\n",
    "</div>\n",
    "\n",
    "A well-known decision tree approach for Machine Learning is the Iterative Dichotomiser 3 (ID3) algorithm. By choosing the best characteristic at each node to partition the data depending on information gain, it recursively constructs a tree. The goal is to make the final subsets as homogeneous as possible. By choosing features that offer the greatest reduction in entropy or uncertainty, ID3 iteratively grows the tree. The procedure keeps going until a halting requirement is satisfied, like a minimum subset size or a maximum tree depth. \n",
    "\n",
    "The ID3 Algorithm is specifically designed for building decision trees from a given dataset. It's primary objective is to construct a tree that best explains the relationship between attributes in the data and their corresponding class labels.\n",
    "\n",
    "**1. Selecting the Best Attribute:**\n",
    "- ID3 employs the concept of entropy and information gain to determine the attribute that best separates the data. Entropy measures the impurity or randomness in the dataset.\n",
    "- The algorithm calculates the entropy of each attribute and selects the one that results in the most significant information gain when used for splitting the data.\n",
    "\n",
    "**2. Creating Tree Nodes:**\n",
    "- The chosen attribute is used to split the dataset into subsets based on its distinct values.\n",
    "- For each subset, ID3 recurses to find the next best attribute to further partition the data, forming branches and new nodes accordingly.\n",
    "\n",
    "**3. Stopping Criteria:**\n",
    "- The recursion continues until one of the stopping criteria is met, such as when all instances in a branch belong to the same class or when all attributes have been used for splitting.\n",
    "\n",
    "**4. Handling Missing Values:**\n",
    "- ID3 can handle missing values to prevent overfitting. While not directly included in ID3, post-processing techniques or variations like C4.5 incorporate pruning to improve the tree's generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Mathematical Concepts of ID3 Algorithm\n",
    "***\n",
    "</div>\n",
    "\n",
    "### Entropy\n",
    "\n",
    "**Entropy** is a measure of disorder or uncertainty in a set of data. It is a tool used in ID3 to measure a dataset's disorder  or impurity. By dividing the data into as homogeneous subsets as feasible, the objective is to minimze entropy.\n",
    "\n",
    "For a set $S$ with classes $\\{c_1,\\space c_2,\\space ...\\space,\\space c_n \\}$, the entropy is calculated as:\n",
    "\n",
    "$$H(S) = \\sum_{i=1}^n \\space p_i \\space log_2(p_i)$$\n",
    "\n",
    "Where $p_i$ is the proportion of instances of class $c_i$ in the set.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information Gain measures how well a certain quality reduces uncertainty. ID3 splits the data at each stage, choosing the property that maximizes Information Gain. It is computes using the distinction between entropy prior to and following the split.\n",
    "\n",
    "Information Gain measures the effectiveness of an Attribute $A$ in reducing uncertainty in set $S$\n",
    "\n",
    "$$IG(A,S) = H(S) - \\sum_{v \\space \\in \\space values(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v))$$\n",
    "\n",
    "Where, $|S_v|$ is the size of the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "### Gain Ratio\n",
    "\n",
    "Gain Ratio is an improvement on Information Gain that considers the inherent worth of characteristics that have a wide range of possible values. It deals with the bias of Information Gan in favor of characteristics with more pronounced values.\n",
    "\n",
    "$$ GR(A,S) = \\frac{IG(A,S)}{\\sum_{v\\space\\in\\space values(A)} \\frac{|S_v|}{|S|} \\cdot log_2(\\frac{|S_v|}{|S|})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem's Resolution Approach\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import (Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        # Feature and Threshold this node was divided with\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Defining the Left and Right children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "        # Value of a Node -> Determines if it is a Node or not\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # If a Node does not have a Value then it is not a Leaf\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        # Amount of Samples needed to perform a split\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "        # Max depth of the decision tree\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Number of features (X) - Helps add some randomness to the Tree\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Defining a root - will later help to traverse the tree\n",
    "        self.root = None\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # Creating a Counter\n",
    "        counter = Counter(y)\n",
    "\n",
    "        # Getting the Most Common Value\n",
    "        value = counter.most_common(1)[0][0]\n",
    "\n",
    "        # Returns most common value\n",
    "        return value\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # The Bincount method creates a numpy array with the occurences of each value.\n",
    "        # The index of the array is the number and it's value in the array corresponds to the amount of times it appears in y\n",
    "        occurences = np.bincount(y)\n",
    "\n",
    "        # Calculating every pi for every X in the previous array\n",
    "        ps = occurences / len(y)\n",
    "\n",
    "        # Returning the Entropy Value\n",
    "        return - np.sum(p * np.log(p) for p in ps if p > 0)\n",
    "\n",
    "    def _split(self, X_Column, split_threshold):\n",
    "        # Splitting the Data\n",
    "        # Note: np.argwhere().flatten() returs the list of indices from the given one where it's elements obey the condition given\n",
    "        left_indices = np.argwhere(X_Column <= split_threshold).flatten()\n",
    "        right_indices = np.argwhere(X_Column > split_threshold).flatten()\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _information_gain(self, y, X_Column, threshold):\n",
    "        # Getting the Parent Entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Create the Children\n",
    "        left_indices, right_indices = self._split(X_Column, threshold)\n",
    "\n",
    "        # Checks if any of the lists are empty\n",
    "        if (left_indices.size == 0 or right_indices.size == 0):\n",
    "            return 0\n",
    "\n",
    "        # -> Calculate the Weighted Average Entropy of the Children\n",
    "\n",
    "        # Number of Samples in y\n",
    "        n = len(y)\n",
    "\n",
    "        # Number of samples in the Left and Right children\n",
    "        n_left, n_right = left_indices.size, right_indices.size\n",
    "\n",
    "        # Calculate the Entropy for both Samples (Left and Right)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
    "\n",
    "        # Calculate the Child Entropy\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        # Calculate Information Gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        # Finds the Best existent split and threshold (Based on the Information Gain)\n",
    "\n",
    "        # Initializing the Best Parameters\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        # Traverse all possible actions\n",
    "        for feat_idx in feature_indices:\n",
    "            X_Column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_Column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the Information Gain\n",
    "                gain = self._information_gain(y, X_Column, threshold)\n",
    "\n",
    "                # Updating the Best Parameters\n",
    "                if (gain > best_gain):\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        # Returning the Best Split Criteria Found\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # Getting the number of samples, features and labels in the data given\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = np.unique(y).size\n",
    "\n",
    "        \"\"\"\n",
    "        # Stopping Criteria\n",
    "\n",
    "        (depth >= self.max_depth)             => Reached Maximum dpeth defined\n",
    "        (n_labels == 1)                       => Current Node only has 1 type of label (which means it's pure)\n",
    "        (n_samples < self.min_samples_split)  => The amount of samples is not enough to perform a split\n",
    "\n",
    "        Therefore, we must return a new node (which is going to be a leaf)\n",
    "        with the current inform\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks the Stopping Criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Getting the Indices of the Features\n",
    "        features_indices = np.random.choide(n_features, self.n_features, replace=False)\n",
    "\n",
    "        # Find the Best Split \n",
    "        best_feature, best_threshold = self._best_split(X, y, features_indices)\n",
    "\n",
    "        # Create Child Nodes (Also makes a recursive call to continue to grow the tree)\n",
    "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
    "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "        def fit(self, X, y):\n",
    "\n",
    "            # Making sure that the amount of features does not surpass the ones available\n",
    "            if not self.n_features:\n",
    "                self.n_features = X.shape[1]\n",
    "            else:\n",
    "                self.n_features = min(X.shape[1], self.n_features)\n",
    "\n",
    "            # Creating a Tree Recursively\n",
    "            self.root = self._grow_tree(X, y)\n",
    "\n",
    "        def _traverse_tree(self, X, node:Node):\n",
    "            # Traverses the Tree until we reached a leaf node -> which will determine the classification label\n",
    "            if (node.is_leaf()):\n",
    "                return node.value\n",
    "\n",
    "            if (X[node.feature] <= node.threshold):\n",
    "                return self._traverse_tree(X, node.left)\n",
    "            else:\n",
    "                return self._traverse_tree(X, node.right)\n",
    "\n",
    "        def predict(self, X):\n",
    "            # Predicts the Label given an Input\n",
    "            return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "        def accuracy(self, y_test, y_predicted):\n",
    "            # Returns the Accuracy of the Model\n",
    "            return np.sum(y_test == y_predicted) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(X, y, test_size=0.3):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise Exception(\"Invalid Test Size Proprotion (Must be between 0 - 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Bibliographic References\n",
    "***\n",
    "</div>\n",
    "\n",
    "1. Geeks For Geeks (2024). *Iteratice Dichotomiser 3 (ID3) Algorithm From Scratch*. Available [here](https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Final Considerations\n",
    "\n",
    "$\\quad$If there is any difficulty on downloading or executing this project, please contact us via:\n",
    "\n",
    "- **Email**:\n",
    "    - [Gonçalo Esteves](https://github.com/EstevesX10) &#8594; `up202203947@up.pt`\n",
    "    - [Maximino Canhola](https://github.com/MaximinoCanhola) &#8594; `up201909805@up.pt`\n",
    "    - [Nuno Gomes](https://github.com/NightF0x26) &#8594; `up202206195@up.pt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
