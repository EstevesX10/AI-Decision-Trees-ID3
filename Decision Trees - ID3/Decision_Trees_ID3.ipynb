{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "---\n",
    "# Decision Trees - ID3 [Artificial Intelligence Project]\n",
    "---\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem Presentation\n",
    "***\n",
    "</div>\n",
    "    \n",
    "> ADD PROBLEM PRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## ID3 Algorithm \n",
    "***\n",
    "</div>\n",
    "\n",
    "A well-known decision tree approach for Machine Learning is the Iterative Dichotomiser 3 (ID3) algorithm. By choosing the best characteristic at each node to partition the data depending on information gain, it recursively constructs a tree. The goal is to make the final subsets as homogeneous as possible. By choosing features that offer the greatest reduction in entropy or uncertainty, ID3 iteratively grows the tree. The procedure keeps going until a halting requirement is satisfied, like a minimum subset size or a maximum tree depth. \n",
    "\n",
    "The ID3 Algorithm is specifically designed for building decision trees from a given dataset. It's primary objective is to construct a tree that best explains the relationship between attributes in the data and their corresponding class labels.\n",
    "\n",
    "**1. Selecting the Best Attribute:**\n",
    "- ID3 employs the concept of entropy and information gain to determine the attribute that best separates the data. Entropy measures the impurity or randomness in the dataset.\n",
    "- The algorithm calculates the entropy of each attribute and selects the one that results in the most significant information gain when used for splitting the data.\n",
    "\n",
    "**2. Creating Tree Nodes:**\n",
    "- The chosen attribute is used to split the dataset into subsets based on its distinct values.\n",
    "- For each subset, ID3 recurses to find the next best attribute to further partition the data, forming branches and new nodes accordingly.\n",
    "\n",
    "**3. Stopping Criteria:**\n",
    "- The recursion continues until one of the stopping criteria is met, such as when all instances in a branch belong to the same class or when all attributes have been used for splitting.\n",
    "\n",
    "**4. Handling Missing Values:**\n",
    "- ID3 can handle missing values to prevent overfitting. While not directly included in ID3, post-processing techniques or variations like C4.5 incorporate pruning to improve the tree's generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Mathematical Concepts of ID3 Algorithm\n",
    "***\n",
    "</div>\n",
    "\n",
    "### Entropy\n",
    "\n",
    "**Entropy** is a measure of disorder or uncertainty in a set of data. It is a tool used in ID3 to measure a dataset's disorder  or impurity. By dividing the data into as homogeneous subsets as feasible, the objective is to minimze entropy.\n",
    "\n",
    "For a set $S$ with classes $\\{c_1,\\space c_2,\\space ...\\space,\\space c_n \\}$, the entropy is calculated as:\n",
    "\n",
    "$$H(S) = \\sum_{i=1}^n \\space p_i \\space log_2(p_i)$$\n",
    "\n",
    "Where $p_i$ is the proportion of instances of class $c_i$ in the set.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information Gain measures how well a certain quality reduces uncertainty. ID3 splits the data at each stage, choosing the property that maximizes Information Gain. It is computes using the distinction between entropy prior to and following the split.\n",
    "\n",
    "Information Gain measures the effectiveness of an Attribute $A$ in reducing uncertainty in set $S$\n",
    "\n",
    "$$IG(A,S) = H(S) - \\sum_{v \\space \\in \\space values(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v))$$\n",
    "\n",
    "Where, $|S_v|$ is the size of the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "### Gain Ratio (Used more in the C4.5 Algorithm)\n",
    "\n",
    "Gain Ratio is an improvement on Information Gain that considers the inherent worth of characteristics that have a wide range of possible values. It deals with the bias of Information Gan in favor of characteristics with more pronounced values.\n",
    "\n",
    "$$ GR(A,S) = \\frac{IG(A,S)}{\\sum_{v\\space\\in\\space values(A)} \\frac{|S_v|}{|S|} \\cdot log_2(\\frac{|S_v|}{|S|})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem's Resolution Approach\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import (Counter)\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Decision Tree - ID3 [Class]\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, information_gain=None, left=None, right=None, *, value=None):\n",
    "        # Feature and Threshold this node was divided with\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.information_gain = information_gain\n",
    "        \n",
    "        # Defining the Left and Right children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "        # Value of a Node -> Determines if it is a Node or not\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # If a Node does not have a Value then it is not a Leaf\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        # Amount of Samples needed to perform a split\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "        # Max depth of the decision tree\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Number of features (X) - Helps add some randomness to the Tree\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Defining a root - will later help to traverse the tree\n",
    "        self.root = None\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # Creating a Counter\n",
    "        counter = Counter(y)\n",
    "\n",
    "        print(\"[REMOVE LATER] - \", counter.most_common())\n",
    "        \n",
    "        # Getting the Most Common Value\n",
    "        value = counter.most_common(1)[0][0]\n",
    "\n",
    "        # Returns most common value\n",
    "        return value\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # The Bincount method creates a numpy array with the occurences of each value.\n",
    "        # The index of the array is the number and it's value in the array corresponds to the amount of times it appears in y\n",
    "        occurences = np.bincount(y)\n",
    "\n",
    "        # Calculating every pi for every X in the previous array\n",
    "        ps = occurences / len(y)\n",
    "\n",
    "        # Returning the Entropy Value\n",
    "        return - sum(p * np.log(p) for p in ps if p > 0)\n",
    "\n",
    "    def _split(self, X_Column, split_threshold):\n",
    "        # Splitting the Data\n",
    "        # Note: np.argwhere().flatten() returs the list of indices from the given one where it's elements obey the condition given\n",
    "        left_indices = np.argwhere(X_Column <= split_threshold).flatten()\n",
    "        right_indices = np.argwhere(X_Column > split_threshold).flatten()\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _information_gain(self, y, X_Column, threshold):\n",
    "        # Getting the Parent Entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Create the Children\n",
    "        left_indices, right_indices = self._split(X_Column, threshold)\n",
    "\n",
    "        # Checks if any of the lists are empty\n",
    "        if (left_indices.size == 0 or right_indices.size == 0):\n",
    "            return 0\n",
    "\n",
    "        # -> Calculate the Weighted Average Entropy of the Children\n",
    "\n",
    "        # Number of Samples in y\n",
    "        n = len(y)\n",
    "\n",
    "        # Number of samples in the Left and Right children\n",
    "        n_left, n_right = left_indices.size, right_indices.size\n",
    "\n",
    "        # Calculate the Entropy for both Samples (Left and Right)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
    "\n",
    "        # Calculate the Child Entropy\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        # Calculate Information Gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        # Finds the Best existent split and threshold (Based on the Information Gain)\n",
    "\n",
    "        # Initializing the Best Parameters\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        # Traverse all possible actions\n",
    "        for feat_idx in feature_indices:\n",
    "            X_Column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_Column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the Information Gain\n",
    "                gain = self._information_gain(y, X_Column, threshold)\n",
    "\n",
    "                # Updating the Best Parameters\n",
    "                if (gain > best_gain):\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        # Returning the Best Split Criteria Found\n",
    "        return split_idx, split_threshold, best_gain\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # Getting the number of samples, features and labels in the data given\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = np.unique(y).size\n",
    "\n",
    "        \"\"\"\n",
    "        # Stopping Criteria\n",
    "\n",
    "        (depth >= self.max_depth)             => Reached Maximum dpeth defined\n",
    "        (n_labels == 1)                       => Current Node only has 1 type of label (which means it's pure)\n",
    "        (n_samples < self.min_samples_split)  => The amount of samples is not enough to perform a split\n",
    "\n",
    "        Therefore, we must return a new node (which is going to be a leaf)\n",
    "        with the current inform\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks the Stopping Criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            print(y)\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Getting the Indices of the Features\n",
    "        features_indices = np.random.choice(n_features, self.n_features, replace=False)\n",
    "\n",
    "        # Find the Best Split \n",
    "        best_feature, best_threshold, info_gain = self._best_split(X, y, features_indices)\n",
    "\n",
    "        # Create Child Nodes (Also makes a recursive call to continue to grow the tree)\n",
    "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
    "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, info_gain, left, right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Making sure that the amount of features does not surpass the ones available\n",
    "        if not self.n_features:\n",
    "            self.n_features = X.shape[1]\n",
    "        else:\n",
    "            self.n_features = min(X.shape[1], self.n_features)\n",
    "    \n",
    "        # Creating a Tree Recursively\n",
    "        self.root = self._grow_tree(X, y)\n",
    "            \n",
    "    def _traverse_tree(self, X, node:Node):\n",
    "        # Traverses the Tree until we reached a leaf node -> which will determine the classification label\n",
    "        if (node.is_leaf()):\n",
    "            return node.value\n",
    "\n",
    "        if (X[node.feature] <= node.threshold):\n",
    "            return self._traverse_tree(X, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(X, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predicts the Label given an Input\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Dataset [Class]\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, file_path):\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.cols = self.df.columns[1:]\n",
    "        \n",
    "        # self.features_decoders = []\n",
    "        # self._correct_dtypes()\n",
    "        # self._encode_categoricaL_features()\n",
    "    \n",
    "        self.data, self.target, self.y_decoder = self._get_data_target()\n",
    "\n",
    "    def _is_categorical(self, elem):\n",
    "        # Checks if a certain feature is categorical (by inspecting the first element of the array)\n",
    "        return type(elem) == str\n",
    "\n",
    "    \"\"\" THESE 2 METHODS ARE NOT YET BEING USED \"\"\"\n",
    "    def _correct_dtypes(self):\n",
    "        # Corrects the Types of each column in the dataset -> helps to further encode the categorical features\n",
    "        for column in self.cols:\n",
    "            if (self._is_categorical(self.df[column][0])):\n",
    "                self.df[column] = self.df[column].astype('category')\n",
    "\n",
    "    def _encode_categoricaL_features(self):\n",
    "        # Encodes all the categorical features / labels\n",
    "        categorical_feats = self.df.select_dtypes(['category']).columns\n",
    "        self.df[categorical_feats] = self.df[categorical_feats].apply(lambda x: x.cat.codes)\n",
    "    \n",
    "    def _label_encoder(self, array):\n",
    "        # Find Unique Values\n",
    "        unique_labels = np.unique(array)\n",
    "        \n",
    "        # Generate a mapping from label to integer\n",
    "        label_encoder = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        \n",
    "        # Creating a Label Decoder\n",
    "        label_decoder = {idx:label for label, idx in label_encoder.items()}\n",
    "        \n",
    "        # Map the original array to the integer labels\n",
    "        encoded_labels = np.array([label_encoder[label] for label in array])\n",
    "        \n",
    "        return encoded_labels, label_decoder\n",
    "\n",
    "    def _get_data_target(self):\n",
    "        # Defining the Target and Label Columns\n",
    "\n",
    "        # X_Cols starts in 1 because we do not need the ID Column\n",
    "        X_Cols = self.cols[0:-1]\n",
    "        Y_Col = self.cols[-1]\n",
    "    \n",
    "        # Splitting the Dataframe into features and label\n",
    "        X = self.df[X_Cols].to_numpy()\n",
    "        y, y_decoder = self._label_encoder(self.df[Y_Col].squeeze().to_numpy())\n",
    "        \n",
    "        return X, y, y_decoder\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        # Note: The array[rand] actually calls the special method __getitem__\n",
    "\n",
    "        # Creating a new order\n",
    "        rand = np.arange(len(self.data))\n",
    "        np.random.shuffle(rand)\n",
    "            \n",
    "        # Rearranges the data / target arrays\n",
    "        self.data = self.data[rand]\n",
    "        self.target = self.target[rand]\n",
    "\n",
    "    def train_test_split(self, test_size=0.3):\n",
    "        # Check if the test_size if valid\n",
    "        if test_size > 1 or test_size < 0:\n",
    "            raise Exception(\"Invalid Test Size Proprotion (Must be between 0 - 1)\")\n",
    "\n",
    "        # Shuffles the Data\n",
    "        self._shuffle_data()\n",
    "\n",
    "        # Defining the training size\n",
    "        train_size = int((1 - test_size) * self.target.size)\n",
    "        \n",
    "        # Splitting the data into training and testing sets\n",
    "        X_Train, X_Test = self.data[:train_size, :], self.data[train_size :, :]\n",
    "        y_Train, y_Test = self.target[:train_size], self.target[train_size :]\n",
    "\n",
    "        # Returning the sets\n",
    "        return X_Train, X_Test, y_Train, y_Test\n",
    "\n",
    "    def _Calculate_Accuracy(self, y_Test, y_Predicted):\n",
    "        # Calculates the Accuracy given the predictions and their actual values\n",
    "        return sum(y_Test == y_Predicted) / len(y_Test)\n",
    "    \n",
    "    \"\"\" Estimate Holdout \"\"\"\n",
    "    def Estimate_Holdout(self, model=DecisionTree, test_size=0.3, *args, **kwargs):\n",
    "        # Creating a new Model\n",
    "        dt = model(*args, **kwargs)\n",
    "\n",
    "        # Splitting the Data\n",
    "        X_Train, X_Test, y_Train, y_Test = self.train_test_split(test_size)\n",
    "\n",
    "        # Train the Model\n",
    "        dt.fit(X_Train, Y_Train)\n",
    "\n",
    "        # Make Predictions\n",
    "        y_Predicted = dt.predict(X_Test)\n",
    "        \n",
    "        # Calculates and Returns the Accuracy of the Model\n",
    "        return self._Calculate_Accuracy(y_Test, y_Predicted)\n",
    "    \n",
    "    def K_Fold_CV(self, total_folds=3, model=DecisionTree, *args, **kwargs):\n",
    "        # Performs a K-Fold Cross Validation\n",
    "\n",
    "        # Length of the Data\n",
    "        n = self.target.size\n",
    "\n",
    "        # Number of folds to perform\n",
    "        k = total_folds\n",
    "\n",
    "        # nfold -> size / length of each subset / fold\n",
    "        nfold = n // k\n",
    "    \n",
    "        # List to store all the calculated accuracies\n",
    "        accuracies = []\n",
    "\n",
    "        # Getting the indices for the data (will have as many as the length of the dataset)\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for i in range(k):\n",
    "            # Getting the test / train indices of the current fold\n",
    "            test_indices = indices[i*nfold : (i+1)*nfold]\n",
    "            train_indices = np.concatenate([indices[: i * nfold], indices[(i + 1) * nfold:]])\n",
    "\n",
    "            # Splitting the data for each new fold\n",
    "            X_Train, y_Train = self.data[train_indices], self.target[train_indices]\n",
    "            X_Test, y_Test = self.data[test_indices], self.target[test_indices]\n",
    "\n",
    "            # Trainning and Evaluating the Model for each new fold\n",
    "            new_model = model(*args, **kwargs) \n",
    "            new_model.fit(X_Train, y_Train)\n",
    "            predictions = new_model.predict(X_Test)\n",
    "            accuracies.append(self._Calculate_Accuracy(y_Test, predictions))\n",
    "\n",
    "        # Returning the average accuracy obtained\n",
    "        return np.mean(accuracies)\n",
    "    \n",
    "    def print_tree(self, dt, node=None, indent=\" \"):\n",
    "        if (dt.root is None):\n",
    "            raise Exception(\"Unfit Model!!!\")\n",
    "        \n",
    "        # Checks if the Node was given\n",
    "        if not node:\n",
    "            node = dt.root\n",
    "\n",
    "        # Found a Leaf / Pure Node\n",
    "        if node.value is not None:\n",
    "            print(self.y_decoder[node.value])\n",
    "\n",
    "        # Shows the feature and threshold of the current node\n",
    "        else:\n",
    "            if (not self._is_categorical(self.data[0 , node.feature])):\n",
    "                print(f\"'{self.cols[node.feature]}' <= {node.threshold} ? [IG:{node.information_gain*100: 2.3f}%]\")\n",
    "            else:\n",
    "                print(f\"Is '{self.cols[node.feature]}' {node.threshold} ? [IG:{node.information_gain*100: 2.3f}%]\")\n",
    "\n",
    "            # Recursive Call to the rest of the tree\n",
    "            print(\"%sleft: \" % (indent), end=\"\")\n",
    "            self.print_tree(dt, node.left, 2*indent)\n",
    "            print(\"%sright: \" % (indent), end=\"\")\n",
    "            self.print_tree(dt, node.right, 2*indent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Model Evaluation with the Datasets\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Hun</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Res</th>\n",
       "      <th>Type</th>\n",
       "      <th>Est</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>30-60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Some</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Burger</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>10-30</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Full</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>&gt;60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Alt  Bar  Fri  Hun   Pat Price Rain  Res    Type    Est Class\n",
       "0  X1  Yes   No   No  Yes  Some   $$$   No  Yes  French   0-10   Yes\n",
       "1  X2  Yes   No   No  Yes  Full     $   No   No    Thai  30-60    No\n",
       "2  X3   No  Yes   No   No  Some     $   No   No  Burger   0-10   Yes\n",
       "3  X4  Yes   No  Yes  Yes  Full     $   No   No    Thai  10-30   Yes\n",
       "4  X5  Yes   No  Yes   No  Full   $$$   No  Yes  French    >60    No"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./Datasets/restaurant.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 6) \n",
      "\n",
      "ID               int64\n",
      "sepallength    float64\n",
      "sepalwidth     float64\n",
      "petallength    float64\n",
      "petalwidth     float64\n",
      "class           object\n",
      "dtype: object \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sepallength</th>\n",
       "      <th>sepalwidth</th>\n",
       "      <th>petallength</th>\n",
       "      <th>petalwidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  sepallength  sepalwidth  petallength  petalwidth        class\n",
       "0   1          5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1   2          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2   3          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3   4          4.6         3.1          1.5         0.2  Iris-setosa\n",
       "4   5          5.0         3.6          1.4         0.2  Iris-setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant = Dataset(file_path='./Datasets/iris.csv')\n",
    "print(restaurant.df.shape, \"\\n\")\n",
    "print(restaurant.df.dtypes, \"\\n\")\n",
    "restaurant.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# restaurant.data\n",
    "# restaurant.target\n",
    "# restaurant.y_decoder\n",
    "# restaurant.cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_Train, X_Test, Y_Train, Y_Test) = restaurant.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 6)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 4)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[2 2 2]\n",
      "[REMOVE LATER] -  [(2, 3)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[0 0]\n",
      "[REMOVE LATER] -  [(0, 2)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[1 1]\n",
      "[REMOVE LATER] -  [(1, 2)]\n",
      "[1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 5)]\n",
      "[0 0 0 0 0]\n",
      "[REMOVE LATER] -  [(0, 5)]\n",
      "[1 1]\n",
      "[REMOVE LATER] -  [(1, 2)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[2 2 2]\n",
      "[REMOVE LATER] -  [(2, 3)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[0]\n",
      "[REMOVE LATER] -  [(0, 1)]\n",
      "[1 0]\n",
      "[REMOVE LATER] -  [(1, 1), (0, 1)]\n",
      "[]\n",
      "[REMOVE LATER] -  []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrestaurant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEstimate_Holdout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m, in \u001b[0;36mDataset.Estimate_Holdout\u001b[1;34m(self, model, test_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m X_Train, X_Test, y_Train, y_Test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Make Predictions\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_Predicted \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mpredict(X_Test)\n",
      "Cell \u001b[1;32mIn[3], line 142\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Creating a Tree Recursively\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[0;32m    129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_indices, :], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "Cell \u001b[1;32mIn[3], line 129\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Create Child Nodes (Also makes a recursive call to continue to grow the tree)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[1;32m--> 129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_indices, :], y[right_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "Cell \u001b[1;32mIn[3], line 129\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Create Child Nodes (Also makes a recursive call to continue to grow the tree)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[1;32m--> 129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_indices, :], y[right_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[0;32m    129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_indices, :], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "    \u001b[1;31m[... skipping similar frames: DecisionTree._grow_tree at line 129 (88 times), DecisionTree._grow_tree at line 130 (6 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 129\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Create Child Nodes (Also makes a recursive call to continue to grow the tree)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[1;32m--> 129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[right_indices, :], y[right_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "Cell \u001b[1;32mIn[3], line 130\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    128\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[0;32m    129\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grow_tree(X[left_indices, :], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(best_feature, best_threshold, info_gain, left, right)\n",
      "Cell \u001b[1;32mIn[3], line 118\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mor\u001b[39;00m n_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(y)\n\u001b[1;32m--> 118\u001b[0m     leaf_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_most_common_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39mleaf_value)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Getting the Indices of the Features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mDecisionTree._most_common_label\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[REMOVE LATER] - \u001b[39m\u001b[38;5;124m\"\u001b[39m, counter\u001b[38;5;241m.\u001b[39mmost_common())\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Getting the Most Common Value\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mcounter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_common\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Returns most common value\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "restaurant.Estimate_Holdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[REMOVE LATER] -  [(0, 33)]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 29)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[2 2 2 2]\n",
      "[REMOVE LATER] -  [(2, 4)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[REMOVE LATER] -  [(2, 31)]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "[REMOVE LATER] -  [(0, 38)]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 31)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[REMOVE LATER] -  [(2, 25)]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[REMOVE LATER] -  [(0, 29)]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 26)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[1 1 1 1 1 1 1 1]\n",
      "[REMOVE LATER] -  [(1, 8)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[1 1]\n",
      "[REMOVE LATER] -  [(1, 2)]\n",
      "[2]\n",
      "[REMOVE LATER] -  [(2, 1)]\n",
      "[1]\n",
      "[REMOVE LATER] -  [(1, 1)]\n",
      "[2 2]\n",
      "[REMOVE LATER] -  [(2, 2)]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[REMOVE LATER] -  [(2, 29)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333332"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant.K_Fold_CV(3, DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree()\n",
    "dt.fit(X_Train, Y_Train)\n",
    "restaurant.print_tree(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Trials [REMOVE LATER]\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (datasets)\n",
    "from sklearn.model_selection import (train_test_split)\n",
    "from sklearn.metrics import (accuracy_score)\n",
    "from sklearn.tree import (DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df[Y_Col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Datasets/iris.csv')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "cols = df.columns[1:]\n",
    "X_Cols = cols[:-1]\n",
    "Y_Col = cols[-1]\n",
    "\n",
    "encoder = {}\n",
    "idx = 0\n",
    "for class_ in df[Y_Col].unique():\n",
    "    encoder.update({class_:idx})\n",
    "    idx += 1\n",
    "    \n",
    "df[Y_Col] = df[Y_Col].map(encoder)\n",
    "\n",
    "X = df[X_Cols]\n",
    "y = df[Y_Col]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(x_train, y_train)\n",
    "dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Just for Guidance [REMOVE LATER]\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()\n",
    "X, Y = data.data, data.target\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_Train, Y_Train)\n",
    "predictions = dt.predict(X_Test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(Y_Test, predictions)\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data['data'].size)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_wine()\n",
    "print(data['data'].size)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Advantages and Disadvantages of ID3\n",
    "***\n",
    "</div>\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "- **Interpretability**: Decision Trees generated by ID3 are **easily interpretable**, making them usefull for explaining decisions to non-technical stakeholders\n",
    "- **Handles Categorical Data**: ID3 can effectively **handle categorical attributes** without explicit data preprocessing steps\n",
    "- **Not Computationally Expensive**: The Algorithm is relatively straightforward and **computationally less expensive** compared to some complex models\n",
    "\n",
    "### **Disadvantages**\n",
    "\n",
    "- **Overfitting**: ID3 tends to create complex trees that may **overfit over the training data**, impacting its performance upon new unseen information\n",
    "- **Sensitive to Noise**: Noise or outliers in the data can lead to the **creation of non-optimal or incorrect splits**\n",
    "- **Exclusive to Binary Trees**: ID3 only constructs **binary trees** which **limits** its ability to **express more complex relationships** within the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Conclusion\n",
    "***\n",
    "</div>\n",
    "\n",
    "The **ID3 Algorithm** laid the groundwork for **decision tree learning**, providing a robust framework for understanding **attribute selection** and **recursive partitioning**. Despite its limitations, ID3's simplicity and interpretability have paved the way for more sophisticated algorithms that address its drawbacks while retaining its essence.\n",
    "\n",
    "As **Machine Learning** continues to evolve, the ID3 Algorithm remains a **crucial piece** in the mosaic of tree-based methods, serving as a stepping stone for developing **more advanced and accurate models** in the quest for **efficient data analysis and pattern recognition**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Bibliographic References\n",
    "***\n",
    "</div>\n",
    "\n",
    "1. Geeks For Geeks (2023). *Decision Tree Algorithms*. Available [here](https://www.geeksforgeeks.org/decision-tree-algorithms/#id3-iterative-dichotomiser-3)\n",
    "2. Geeks For Geeks (2024). *Iteratice Dichotomiser 3 (ID3) Algorithm From Scratch*. Available [here](https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Video Presentation (YouTube Video)\n",
    "\n",
    "Moreover, you can find the **Video** for our **Project's Presentation [here](https://youtu.be/dQw4w9WgXcQ?si=NfwpqDkOwLRY6tRQ)**\n",
    "___\n",
    "## Final Considerations\n",
    "\n",
    "$\\quad$ If there is any difficulty on downloading or executing this project, please contact us via:\n",
    "\n",
    "- **Email**:\n",
    "    - [Gonçalo Esteves](https://github.com/EstevesX10) &#8594; `up202203947@up.pt`\n",
    "    - [Maximino Canhola](https://github.com/MaximinoCanhola) &#8594; `up201909805@up.pt`\n",
    "    - [Nuno Gomes](https://github.com/NightF0x26) &#8594; `up202206195@up.pt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
