{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "---\n",
    "# Decision Trees - ID3 [Artificial Intelligence Project]\n",
    "---\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem Presentation\n",
    "***\n",
    "</div>\n",
    "    \n",
    "> ADD PROBLEM PRESENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## ID3 Algorithm \n",
    "***\n",
    "</div>\n",
    "\n",
    "A well-known decision tree approach for Machine Learning is the Iterative Dichotomiser 3 (ID3) algorithm. By choosing the best characteristic at each node to partition the data depending on information gain, it recursively constructs a tree. The goal is to make the final subsets as homogeneous as possible. By choosing features that offer the greatest reduction in entropy or uncertainty, ID3 iteratively grows the tree. The procedure keeps going until a halting requirement is satisfied, like a minimum subset size or a maximum tree depth. \n",
    "\n",
    "The ID3 Algorithm is specifically designed for building decision trees from a given dataset. It's primary objective is to construct a tree that best explains the relationship between attributes in the data and their corresponding class labels.\n",
    "\n",
    "**1. Selecting the Best Attribute:**\n",
    "- ID3 employs the concept of entropy and information gain to determine the attribute that best separates the data. Entropy measures the impurity or randomness in the dataset.\n",
    "- The algorithm calculates the entropy of each attribute and selects the one that results in the most significant information gain when used for splitting the data.\n",
    "\n",
    "**2. Creating Tree Nodes:**\n",
    "- The chosen attribute is used to split the dataset into subsets based on its distinct values.\n",
    "- For each subset, ID3 recurses to find the next best attribute to further partition the data, forming branches and new nodes accordingly.\n",
    "\n",
    "**3. Stopping Criteria:**\n",
    "- The recursion continues until one of the stopping criteria is met, such as when all instances in a branch belong to the same class or when all attributes have been used for splitting.\n",
    "\n",
    "**4. Handling Missing Values:**\n",
    "- ID3 can handle missing values to prevent overfitting. While not directly included in ID3, post-processing techniques or variations like C4.5 incorporate pruning to improve the tree's generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Mathematical Concepts of ID3 Algorithm\n",
    "***\n",
    "</div>\n",
    "\n",
    "### Entropy\n",
    "\n",
    "**Entropy** is a measure of disorder or uncertainty in a set of data. It is a tool used in ID3 to measure a dataset's disorder  or impurity. By dividing the data into as homogeneous subsets as feasible, the objective is to minimze entropy.\n",
    "\n",
    "For a set $S$ with classes $\\{c_1,\\space c_2,\\space ...\\space,\\space c_n \\}$, the entropy is calculated as:\n",
    "\n",
    "$$H(S) = \\sum_{i=1}^n \\space p_i \\space log_2(p_i)$$\n",
    "\n",
    "Where $p_i$ is the proportion of instances of class $c_i$ in the set.\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "Information Gain measures how well a certain quality reduces uncertainty. ID3 splits the data at each stage, choosing the property that maximizes Information Gain. It is computes using the distinction between entropy prior to and following the split.\n",
    "\n",
    "Information Gain measures the effectiveness of an Attribute $A$ in reducing uncertainty in set $S$\n",
    "\n",
    "$$IG(A,S) = H(S) - \\sum_{v \\space \\in \\space values(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v))$$\n",
    "\n",
    "Where, $|S_v|$ is the size of the subset of $S$ for which attribute $A$ has value $v$.\n",
    "\n",
    "### Gain Ratio\n",
    "\n",
    "Gain Ratio is an improvement on Information Gain that considers the inherent worth of characteristics that have a wide range of possible values. It deals with the bias of Information Gan in favor of characteristics with more pronounced values.\n",
    "\n",
    "$$ GR(A,S) = \\frac{IG(A,S)}{\\sum_{v\\space\\in\\space values(A)} \\frac{|S_v|}{|S|} \\cdot log_2(\\frac{|S_v|}{|S|})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Problem's Resolution Approach\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import (Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        # Feature and Threshold this node was divided with\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Defining the Left and Right children\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "        # Value of a Node -> Determines if it is a Node or not\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # If a Node does not have a Value then it is not a Leaf\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        # Amount of Samples needed to perform a split\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "        # Max depth of the decision tree\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # Number of features (X) - Helps add some randomness to the Tree\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Defining a root - will later help to traverse the tree\n",
    "        self.root = None\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # Creating a Counter\n",
    "        counter = Counter(y)\n",
    "\n",
    "        # Getting the Most Common Value\n",
    "        value = counter.most_common(1)[0][0]\n",
    "\n",
    "        # Returns most common value\n",
    "        return value\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # The Bincount method creates a numpy array with the occurences of each value.\n",
    "        # The index of the array is the number and it's value in the array corresponds to the amount of times it appears in y\n",
    "        occurences = np.bincount(y)\n",
    "\n",
    "        # Calculating every pi for every X in the previous array\n",
    "        ps = occurences / len(y)\n",
    "\n",
    "        # Returning the Entropy Value\n",
    "        return - sum(p * np.log(p) for p in ps if p > 0)\n",
    "\n",
    "    def _split(self, X_Column, split_threshold):\n",
    "        # Splitting the Data\n",
    "        # Note: np.argwhere().flatten() returs the list of indices from the given one where it's elements obey the condition given\n",
    "        left_indices = np.argwhere(X_Column <= split_threshold).flatten()\n",
    "        right_indices = np.argwhere(X_Column > split_threshold).flatten()\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _information_gain(self, y, X_Column, threshold):\n",
    "        # Getting the Parent Entropy\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Create the Children\n",
    "        left_indices, right_indices = self._split(X_Column, threshold)\n",
    "\n",
    "        # Checks if any of the lists are empty\n",
    "        if (left_indices.size == 0 or right_indices.size == 0):\n",
    "            return 0\n",
    "\n",
    "        # -> Calculate the Weighted Average Entropy of the Children\n",
    "\n",
    "        # Number of Samples in y\n",
    "        n = len(y)\n",
    "\n",
    "        # Number of samples in the Left and Right children\n",
    "        n_left, n_right = left_indices.size, right_indices.size\n",
    "\n",
    "        # Calculate the Entropy for both Samples (Left and Right)\n",
    "        entropy_left, entropy_right = self._entropy(y[left_indices]), self._entropy(y[right_indices])\n",
    "\n",
    "        # Calculate the Child Entropy\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        # Calculate Information Gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _best_split(self, X, y, feature_indices):\n",
    "        # Finds the Best existent split and threshold (Based on the Information Gain)\n",
    "\n",
    "        # Initializing the Best Parameters\n",
    "        best_gain = -1\n",
    "        split_idx, split_threshold = None, None\n",
    "\n",
    "        # Traverse all possible actions\n",
    "        for feat_idx in feature_indices:\n",
    "            X_Column = X[:, feat_idx]\n",
    "            thresholds = np.unique(X_Column)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                # Calculate the Information Gain\n",
    "                gain = self._information_gain(y, X_Column, threshold)\n",
    "\n",
    "                # Updating the Best Parameters\n",
    "                if (gain > best_gain):\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        # Returning the Best Split Criteria Found\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # Getting the number of samples, features and labels in the data given\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = np.unique(y).size\n",
    "\n",
    "        \"\"\"\n",
    "        # Stopping Criteria\n",
    "\n",
    "        (depth >= self.max_depth)             => Reached Maximum dpeth defined\n",
    "        (n_labels == 1)                       => Current Node only has 1 type of label (which means it's pure)\n",
    "        (n_samples < self.min_samples_split)  => The amount of samples is not enough to perform a split\n",
    "\n",
    "        Therefore, we must return a new node (which is going to be a leaf)\n",
    "        with the current inform\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks the Stopping Criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Getting the Indices of the Features\n",
    "        features_indices = np.random.choice(n_features, self.n_features, replace=False)\n",
    "\n",
    "        # Find the Best Split \n",
    "        best_feature, best_threshold = self._best_split(X, y, features_indices)\n",
    "\n",
    "        # Create Child Nodes (Also makes a recursive call to continue to grow the tree)\n",
    "        left_indices, right_indices = self._split(X[:, best_feature], best_threshold)\n",
    "        left = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n",
    "        right = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n",
    "        \n",
    "        return Node(best_feature, best_threshold, left, right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Making sure that the amount of features does not surpass the ones available\n",
    "        if not self.n_features:\n",
    "            self.n_features = X.shape[1]\n",
    "        else:\n",
    "            self.n_features = min(X.shape[1], self.n_features)\n",
    "    \n",
    "        # Creating a Tree Recursively\n",
    "        self.root = self._grow_tree(X, y)\n",
    "            \n",
    "    def _traverse_tree(self, X, node:Node):\n",
    "        # Traverses the Tree until we reached a leaf node -> which will determine the classification label\n",
    "        if (node.is_leaf()):\n",
    "            return node.value\n",
    "\n",
    "        if (X[node.feature] <= node.threshold):\n",
    "            return self._traverse_tree(X, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(X, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predicts the Label given an Input\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def accuracy(self, y_test, y_predicted):\n",
    "        # Returns the Accuracy of the Model\n",
    "        return sum(y_test == y_predicted) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.3):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise Exception(\"Invalid Test Size Proprotion (Must be between 0 - 1)\")\n",
    "\n",
    "    # NOT THE RIGHT APPROACH \n",
    "    shuffled_df = df.sample(frac=1)\n",
    "\n",
    "    train_size = int((1-test_size) * len(df))\n",
    "\n",
    "    train_set = shuffled_df[:train_size]\n",
    "    test_set = shuffled_df[train_size:]\n",
    "\n",
    "    X_Train, Y_Train = np.array(train_set.iloc[: ,:-1]), np.array(train_set.iloc[:, -1:])\n",
    "    X_Test, Y_Test = np.array(test_set.iloc[:, :-1]), np.array(test_set.iloc[:, -1:])\n",
    "\n",
    "    return X_Train, X_Test, Y_Train, Y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "def _indexing(x, indices):\n",
    "    \"\"\"\n",
    "    :param x: array from which indices has to be fetched\n",
    "    :param indices: indices to be fetched\n",
    "    :return: sub-array from given array and indices\n",
    "    \"\"\"\n",
    "    # np array indexing\n",
    "    if hasattr(x, 'shape'):\n",
    "        return x[indices]\n",
    "\n",
    "    # list indexing\n",
    "    return [x[idx] for idx in indices]\n",
    "\n",
    "def train_test_split_V2(*arrays, test_size=0.25, shufffle=True, random_seed=1):\n",
    "    \"\"\"\n",
    "    splits array into train and test data.\n",
    "    :param arrays: arrays to split in train and test\n",
    "    :param test_size: size of test set in range (0,1)\n",
    "    :param shufffle: whether to shuffle arrays or not\n",
    "    :param random_seed: random seed value\n",
    "    :return: return 2*len(arrays) divided into train ans test\n",
    "    \"\"\"\n",
    "    # checks\n",
    "    assert 0 < test_size < 1\n",
    "    assert len(arrays) > 0\n",
    "    length = len(arrays[0])\n",
    "    for i in arrays:\n",
    "        assert len(i) == length\n",
    "\n",
    "    n_test = int(np.ceil(length*test_size))\n",
    "    n_train = length - n_test\n",
    "\n",
    "    if shufffle:\n",
    "        perm = np.random.RandomState(random_seed).permutation(length)\n",
    "        test_indices = perm[:n_test]\n",
    "        train_indices = perm[n_test:]\n",
    "    else:\n",
    "        train_indices = np.arange(n_train)\n",
    "        test_indices = np.arange(n_train, length)\n",
    "\n",
    "    return list(chain.from_iterable((_indexing(x, train_indices), _indexing(x, test_indices)) for x in arrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Datasets\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Alt</th>\n",
       "      <th>Bar</th>\n",
       "      <th>Fri</th>\n",
       "      <th>Hun</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Res</th>\n",
       "      <th>Type</th>\n",
       "      <th>Est</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>30-60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X3</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Some</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Burger</td>\n",
       "      <td>0-10</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Full</td>\n",
       "      <td>$</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Thai</td>\n",
       "      <td>10-30</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Full</td>\n",
       "      <td>$$$</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>French</td>\n",
       "      <td>&gt;60</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Alt  Bar  Fri  Hun   Pat Price Rain  Res    Type    Est Class\n",
       "0  X1  Yes   No   No  Yes  Some   $$$   No  Yes  French   0-10   Yes\n",
       "1  X2  Yes   No   No  Yes  Full     $   No   No    Thai  30-60    No\n",
       "2  X3   No  Yes   No   No  Some     $   No   No  Burger   0-10   Yes\n",
       "3  X4  Yes   No  Yes  Yes  Full     $   No   No    Thai  10-30   Yes\n",
       "4  X5  Yes   No  Yes   No  Full   $$$   No  Yes  French    >60    No"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_df = pd.read_csv(\"./Datasets/restaurant.csv\")\n",
    "restaurant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(restaurant_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['X8', 'No', 'No', 'No', 'Yes', 'Some', '$$', 'Yes', 'Yes',\n",
       "        'Thai', '0-10'],\n",
       "       ['X9', 'No', 'Yes', 'Yes', 'No', 'Full', '$', 'Yes', 'No',\n",
       "        'Burger', '>60'],\n",
       "       ['X5', 'Yes', 'No', 'Yes', 'No', 'Full', '$$$', 'No', 'Yes',\n",
       "        'French', '>60'],\n",
       "       ['X7', 'No', 'Yes', 'No', 'No', 'None', '$', 'Yes', 'No',\n",
       "        'Burger', '0-10'],\n",
       "       ['X3', 'No', 'Yes', 'No', 'No', 'Some', '$', 'No', 'No', 'Burger',\n",
       "        '0-10'],\n",
       "       ['X10', 'Yes', 'Yes', 'Yes', 'Yes', 'Full', '$$$', 'No', 'Yes',\n",
       "        'Italian', '10-30'],\n",
       "       ['X4', 'Yes', 'No', 'Yes', 'Yes', 'Full', '$', 'No', 'No', 'Thai',\n",
       "        '10-30'],\n",
       "       ['X1', 'Yes', 'No', 'No', 'Yes', 'Some', '$$$', 'No', 'Yes',\n",
       "        'French', '0-10']], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_Train.shape)\n",
    "X_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTree()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mpredict(X_Test)\n",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Creating a Tree Recursively\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grow_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m, in \u001b[0;36mDecisionTree._grow_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m    119\u001b[0m features_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(n_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Find the Best Split \u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m best_feature, best_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Create Child Nodes (Also makes a recursive call to continue to grow the tree)\u001b[39;00m\n\u001b[0;32m    125\u001b[0m left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m, in \u001b[0;36mDecisionTree._best_split\u001b[1;34m(self, X, y, feature_indices)\u001b[0m\n\u001b[0;32m     82\u001b[0m thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X_Column)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Calculate the Information Gain\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_information_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_Column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# Updating the Best Parameters\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (gain \u001b[38;5;241m>\u001b[39m best_gain):\n",
      "Cell \u001b[1;32mIn[3], line 45\u001b[0m, in \u001b[0;36mDecisionTree._information_gain\u001b[1;34m(self, y, X_Column, threshold)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_information_gain\u001b[39m(\u001b[38;5;28mself\u001b[39m, y, X_Column, threshold):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Getting the Parent Entropy\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     parent_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Create the Children\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     left_indices, right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X_Column, threshold)\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mDecisionTree._entropy\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_entropy\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# The Bincount method creates a numpy array with the occurences of each value.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# The index of the array is the number and it's value in the array corresponds to the amount of times it appears in y\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     occurences \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Calculating every pi for every X in the previous array\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     ps \u001b[38;5;241m=\u001b[39m occurences \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree()\n",
    "dt.fit(X_Train, Y_Train)\n",
    "predictions = dt.predict(X_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Just for Guidance [REMOVE LATER]\n",
    "***\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (datasets)\n",
    "from sklearn.model_selection import (train_test_split)\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X, Y = data.data, data.target\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state=1234)\n",
    "\n",
    "dt = DecisionTree()\n",
    "dt.fit(X_Train, Y_Train)\n",
    "predictions = dt.predict(X_Test)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(Y_Test, predictions)\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "***\n",
    "## Bibliographic References\n",
    "***\n",
    "</div>\n",
    "\n",
    "1. Geeks For Geeks (2024). *Iteratice Dichotomiser 3 (ID3) Algorithm From Scratch*. Available [here](https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Final Considerations\n",
    "\n",
    "$\\quad$If there is any difficulty on downloading or executing this project, please contact us via:\n",
    "\n",
    "- **Email**:\n",
    "    - [Gon√ßalo Esteves](https://github.com/EstevesX10) &#8594; `up202203947@up.pt`\n",
    "    - [Maximino Canhola](https://github.com/MaximinoCanhola) &#8594; `up201909805@up.pt`\n",
    "    - [Nuno Gomes](https://github.com/NightF0x26) &#8594; `up202206195@up.pt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
